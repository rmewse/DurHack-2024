{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision HackPack\n",
    "In this Hackpack we will be creating an application that recognises objects from a video stream!\n",
    "\n",
    "### Prerequisites\n",
    "* Python 3.8 *Later versions dont play nice with the Simple Online Realtime Tracking (SORT) module*\n",
    "    * You can get this easily by creating a virtual environment. Think of this as a sandbox where you can install python packages in isolation of other ones that may be stored on your computer.\n",
    "      * To do this install miniconda https://docs.anaconda.com/miniconda/miniconda-install/\n",
    "      * Create the environment by ruuning the command *conda create -n hackpack python=3.8 anaconda*\n",
    "      * Activate the environment by running *conda activate hackpack*   \n",
    "* Install the required packages by running *pip install opencv-python*, *pip install numpy* and then *pip install sort-track*\n",
    "* We also require the model, You Only Look Once (YOLO), we will use to detect objects! You can find the models folder here. Place it in the same folder as this notebook.\n",
    "#INSERT LINK HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages you just installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from sort.tracker import SortTracker\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# from sort.tracker import SortTracker\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO\n",
    "net = cv2.dnn.readNet('./models/yolov3.weights', './models/yolov3.cfg')\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = [\"person\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the SORT Tracker.\n",
    "This model compares the positions of objects between frames and tries to guess whther they are the same or different objects. If they are different we will draw a new bounding box around it with a new id. If we guess that it's the same then we move the bounding box with that ID from the last frame to surround the object at its new location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = SortTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the video capture\n",
    "Here we use OpenCV2 to start a video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # ID of the camera being used, try and change this if it's not working or you have more than one camera\n",
    "width  = cap.get(3)  \n",
    "height = cap.get(4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot loop ðŸ”¥ðŸ”¥\n",
    "\n",
    "This is where the bulk of the processing happens but in summary it:\n",
    "<ol>\n",
    "<li> Captures a frame\n",
    "<li> Puts it through the YOLO model\n",
    "<li> For each object detected. If we are sure enough that it exists then we work out a Bounding Box for it\n",
    "<li> We pass the Bounding Boxes to the SORT module which determines if its a new object\n",
    "<li> We pass the boxes along with their IDS to be drawn on the next frame\n",
    "</ol>\n",
    "Hold q to stop the capture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Capture frame-by-frame 1\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to a blob and pass through the network 2\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "   \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "\n",
    "    # Analyze the outs array 3\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            \n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Convert YOLO detections to SORT format (x, y, w, h) 4\n",
    "    detections = []\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            detections.append([x, y, x + w, y + h,1,1])\n",
    "\n",
    "    # Update SORT tracker with current frame detections 4\n",
    "    try:\n",
    "        track_bbs_ids = tracker.update(np.array(detections),\"\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    # Drawing tracked objects on the image 5\n",
    "    items = 0\n",
    "    for track in track_bbs_ids:\n",
    "    \n",
    "        x, y, w, h, track_id,_,_ = track\n",
    "        cv2.rectangle(frame, (int(x), int(y)), (int(w), int(h)), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f\"ID {track_id}\", (int(x), int(y - 10)), 0, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the resulting frame 5\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        \n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
